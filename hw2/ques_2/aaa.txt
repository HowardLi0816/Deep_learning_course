import numpy as np
from dataset import get_data
from k_mean import k_means, to_one_hot
import math
import pandas as pd
import random

def joint_Gauss(x, mean_k, cov_k):
    d = x.shape[0]
    #print(d)
    cov_inv = np.linalg.inv(cov_k)
    up = np.exp(-1/2 * (x-mean_k).dot(cov_inv).dot(x-mean_k))
    down = np.sqrt((2*math.pi)**d * np.linalg.det(cov_k))
    prob = up / down
    #print(prob)
    return prob

def e_step_sub(x, w, mean, cov):
    '''
    x: (2,)
    w: (3,)
    mean: (3, 2)
    cov: (3, 2, 2)
    '''
    k = w.shape[0]
    rk = np.zeros(k)
    f_xn = np.zeros(k)
    for i in range(k):
        #print(joint_Gauss(x, mean[i], cov[i]))
        f_xn[i] = w[i] * joint_Gauss(x, mean[i], cov[i])
    down = f_xn.sum()
    for i in range(k):
        rk[i] = f_xn[i] / down
    return rk

def e_step(x_all, w, mean, cov):
    '''
    x_all: (N, 2)
    w: (3,)
    mean: (3, 2)
    cov: (3, 2, 2)
    '''
    N = x_all.shape[0]
    k = w.shape[0]
    rk_all = np.zeros((N, k))
    for n in range(N):
        rk_all[n] = e_step_sub(x_all[n], w, mean, cov)
    return rk_all

def m_step(x_all, rk_all):
    '''

    :param x_all: (N, 2)
    :param rk_all: (N, 3)
    :return:
    '''
    #print(x_all)
    N = x_all.shape[0]
    d = x_all.shape[1]
    k = rk_all.shape[1]
    #print(rk_all)
    meank_new = np.zeros((k, d))
    covk_new = np.zeros((k, d, d))
    wk_new = np.zeros(k)
    for i in range(k):
        up = np.zeros(2)
        down = rk_all[:, i].sum()
        for n in range(N):
            up += rk_all[n][i] * x_all[i]
        meank_new[i] = up/down
        #print('aaaaa',meank_new[i])

        up_1 = np.zeros((d, d))
        for n in range(N):
            xn_uk = x_all[n] - meank_new[i]
            #print(xn_uk)
            x1 = xn_uk.reshape(d, 1)
            x2 = xn_uk.reshape(1, d)
            do = x1.dot(x2)
            up_1 += rk_all[n][i] * do
        covk_new[i] = up_1/down
        #print('bbbbb', covk_new[i])

        wk_new[i] = down/N

    return meank_new, covk_new, wk_new

def neg_log_likelihood(x_all, w_all, mean_all, cov_all):
    '''
    :param x_all: (N, 2)
    :param w_all: (3, )
    :param mean_all: (3, 2)
    :param cov_all: (3, 2, 2)
    :return:
    '''
    N = x_all.shape[0]
    d = x_all.shape[1]
    k = w_all.shape[0]
    loss = 0
    for n in range(N):
        su = 0
        for i in range(k):
            su = su + w_all[i] * joint_Gauss(x_all[n], mean_all[i], cov_all[i])
            #print(joint_Gauss(x_all[n], mean_all[i], cov_all[i]))
        loss = loss + np.log(su)
    return loss

def initialize(data, K):
    '''
    :param data:(N, 3)
    :param K:
    :return:
    '''
    N = data.shape[0]
    a = 0
    thresh = 0.5
    data_fea = data[:, :-1]
    data_label = data[:, -1]
    while a < thresh:
        center, result = k_means(data_fea, K)
        confusion_matrix = pd.crosstab(data_label, result, rownames=['Actual'], colnames=['Predicted'])
        conf = confusion_matrix.values
        a = np.sum(np.diag(conf, k=0)) / np.sum(conf)
    print(confusion_matrix)
    one_hot = to_one_hot(result, K)

    '''
    one_hot = np.zeros((N, K))
    for i in range(N):
        one_hot[i][0] = 1
    '''
    return one_hot

def gmm(data, thresh, init_rk):
    '''

    :param data: (N, 3)
    :param thresh:
    :param init_rk: (N, 3)
    :return:
    '''
    data_fea = data[:, :-1]
    rk = init_rk
    mean, cov, w = m_step(data_fea, rk)
    #print(rk)
    loss = neg_log_likelihood(data_fea, w, mean, cov)
    #print(loss)
    last_loss = math.inf
    #print(loss)
    iter = 0
    while abs(loss-last_loss) >= thresh:
        last_loss = loss
        iter += 1
        rk = e_step(data_fea, w, mean, cov)
        #print(rk)
        mean, cov, w = m_step(data_fea, rk)
        loss = neg_log_likelihood(data_fea, w, mean, cov)

        #eval
        acc = eval(data, w, mean, cov)

        print('iter:', iter, 'loss:', loss, 'acc:', acc)
    return mean, cov, w

def eval(data, w, mean, cov):
    N = data.shape[0]
    data_fea = data[:, :-1]
    data_label = data[:, -1]
    rk = e_step(data_fea, w, mean, cov)
    result = np.zeros(N)
    for n in range(N):
        rk_sub = rk[n]
        result[n] = rk_sub.argmax()
    #print(result)

    confusion_matrix = pd.crosstab(data_label, result, rownames=['Actual'], colnames=['Predicted'])
    print('confusion matrix:', confusion_matrix)
    conf = confusion_matrix.values
    acc = np.sum(np.diag(conf, k=0)) / np.sum(conf)
    return acc

if __name__ == '__main__':
    #load data
    filename = './cluster.txt'
    print('Start loading dataset from ', filename)
    data = get_data(filename)
    data_head = np.array(data[0])
    data_left = np.array(data[1])
    data_right = np.array(data[2])
    # print(data_head.shape, data_left.shape, data_right.shape)
    data_np = np.concatenate((data_head, data_left, data_right), axis=0)
    print('Finish loading dataset!')

    #Init
    print('Starting initialization!')
    K = 3
    init_rk = initialize(data_np, K)

    #training
    thresh = 1e-5
    print('Starting training with threshold:', thresh)
    mean, cov, w = gmm(data_np, thresh, init_rk)

    #eval
    print('Start eval!')
    acc = eval(data_np, w, mean, cov)
    print('Final acc:', acc)